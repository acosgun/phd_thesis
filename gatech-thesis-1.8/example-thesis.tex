\documentclass[12pt]{gatech-thesis}
\usepackage{amsmath,amssymb,latexsym,float,epsfig,subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{lscape}
%\usepackage[round]{natbib}
\newcolumntype{x}[1]{>{\raggedleft\hspace{0pt}}p{#1}}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
%\title{Behavior Design and Representations For a People Aware Mobile Robot System} %% If you want to specify a linebreak

\title{Navigation Behavior Design and Representations For a People Aware Mobile Robot System}

                               %% in the thesis title, you MUST use
                               %% \protect\\ instead of \\, as \\ is a
                               %% fragile command that \MakeUpperCase
                               %% will break!
\author{Akansel Cosgun}
\department{College of Computing}

%% Can have up to six readers, plus principaladvisor and
%% committeechair. All have the form
%%
%%  \reader{Name}[Department][Institution]
%%
%% The second and third arguments are optional, but if you wish to
%% supply the third, you must supply the second. Department defaults
%% to the department defined above and Institution defaults to Georgia
%% Institute of Technology.

\principaladvisor{Professor Henrik Iskov Christensen}
%\committeechair{Professor Ignatius Arrogant}
\firstreader{Professor Andrea Thomaz}[College of Computing]
\secondreader{Professor Irfan Essa}[College of Computing]
\thirdreader{Professor Ayanna Howard}[School of Electrical and Computer Engineering]
\fourthreader{Dr Emrah Akin Sisbot}[Toyota InfoTechnology Center][]
%\setcounter{secnumdepth}{2}
\degree{Doctor of Philosophy}

%% Set \listmajortrue below, then uncomment and set this for
%% interdisciplinary PhD programs so that the title page says
%% ``[degree] in [major]'' and puts the department at the bottom of
%% the page, rather than saying ``[degree] in the [department]''

%% \major{Algorithms, Combinatorics, and Optimization} 

\copyrightyear{2016} 
\submitdate{May 2016} % Must be the month and year of graduation,
                         % not thesis approval! As of 2010, this means
                         % this text must be May, August, or December
                         % followed by the year.

%% The date the last committee member signs the thesis form. Printed
%% on the approval page.
\approveddate{X/Y/2015}

\bibfiles{example-thesis}

%% The following are the defaults
%%    \titlepagetrue
%%    \signaturepagetrue
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
%%    \bibpagetrue
%%    \thesisproposalfalse
%%    \strictmarginstrue
%%    \dissertationfalse
%%    \listmajorfalse
%%    \multivolumefalse


\begin{document}
%\bibliographystyle{plainnat}
\bibliographystyle{gatech-thesis}
%%
\begin{preliminary}

% Dedication
\begin{dedication}
\null\vfil
{\large
\begin{center}
To my parents,\\\vspace{12pt}
Zeynep and Huseyin Co\c{s}gun,\\\vspace{12pt}
who are my pillars of strength;\\\vspace{12pt}
and to my wife Gamze,\\\vspace{12pt}
who always supports me, rain or shine.\\\vspace{12pt}
\end{center}}
\vfil\null
\end{dedication}


%\begin{preface}
%Preface
%\end{preface}


\include{chapters/acknowledgements}

% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.
\begin{summary}

There are millions of robots in operation around the world today, and almost all of them operate on factory floors in isolation from people. However, it is now becoming clear that robots could provide much more value assisting people in daily tasks in human environments. Perhaps the most fundamental capability for a mobile robot is navigating from one location to another. Advances in mapping and motion planning research in past decades made indoor navigation a commodity for mobile robots. Yet, questions remain on how the robots should move around humans. This thesis advocates the use of semantic maps and spatial rules of engagement to enable non-expert users to effortlessly interact with and control a mobile robot.

A core concept explored in this thesis is the Tour Scenario, where the goal is to familiarize a mobile robot to a new environment after it is first shipped and unpacked at a home or office setting. During the tour, the robot follows the user and creates a semantic representation of the environment. The user labels objects, landmarks and locations by performing pointing gestures and using the robot's user interface. The spatial semantic information is meaningful to humans, as it allows providing goals to the robot such as ``bring me a cup from the kitchen table". While the robot is navigating towards the goal, it should not treat nearby humans as obstacles, therefore shortest-path approaches can fail to be socially acceptable.

Three main navigation behaviors are studied in this work. The first behavior is the point-to-point navigation. The navigation planner presented in this thesis borrows ideas from human-human spatial interactions, and takes into account personal spaces as well as reactions of people who are in close proximity to the trajectory of the robot. The second navigation behavior is person following. After the description of a basic following behavior, a user study on person following for telepresence robots is presented. Additionally, a situation aware following behavior is demonstrated, where the robot can facilitate tasks by using the semantic map and predicting the intent of the user. The third behavior is person guidance. A tour-guide robot is presented with a particular application for visually impaired users.

\end{summary}
\end{preliminary}
%%

%\include{chapters/introduction}

\chapter{Introduction}
\label{chapter:introduction}


\section{Robots in Human Environments}

The vision that was promised by Karel \v{C}apek in his 1921 science fiction play R.U.R, in which the word \textit{``robot"} was used the first time, featured intelligent autonomous systems that co-existed and worked for humans. The reality almost a century later today is that robots do exist, however almost all of them operate in factories, physically separated from human workers. With this separation, the safety of humans is ensured. However, it is now becoming clear that robots can provide much more value if they operate in human environments, assisting people in daily tasks. With the recent improvements in hardware and software systems and in robotics research, development of such robots is not science fiction anymore. This disruptive technology will make fascinating new applications possible, in contexts such as homes, hospitals, offices and factories, including but not limited to: delivery, elderly care, collaborating on an assembly line and household tasks such as cleaning. The feasibility and reliability of such applications will determine their business value, while potentially generating a new industry. Therefore, any development in this field is a step toward realization of \textit{intelligent} and \textit{social} robots.

\section{Robots that Navigate in Human Environments}

The most important difference of a robot from a computer is actuation: the ability to move around in the physical world. Perhaps the most fundamental task for a mobile robot is to be able to \textit{navigate} from a location to another. Advances in mapping, localization and path planning research in the past decades made indoor navigation a common capability for mobile robots. 

Robots occupy the same physical space with us, therefore should be aware of the fact that their movements would be observed and reacted by humans. Investigating the context of robots in human environments gave rise to the sub-field of robotics: Human-Robot Interaction (HRI). This relatively new research field extends in many directions with a common goal: robots that will perceive its environment, reason and act in a safe way to facilitate people's lives. Thus, a robot that will serve to people should not only be a machine, but should respect social rules and protocols.

\section{The Science of Personal Spaces}
\label{sec:personal_spaces}

How should mobile agents adjust their spatial relationships with respect to humans? In fact, humans already have a framework for this problem. It is called the \textit{proxemics}, coined by Hall \cite{hall1966hidden}, which studies of our use of space and the degree and effect of the spatial separation individuals naturally maintain. One of Hall's main findings is that humans adjust their distance in four distance levels: intimate, personal, social and public distances, depending on the intimacy level between individuals.

Personal spaces are influenced by other factors such as cultures, the range of their sensory systems, and postures of the two actors. Although it is not known whether it is best to model robot navigation behaviors after humans, using human-human studies to determine the spatial relationships to humans is a good start. This is especially true given that non-expert users today never interacted with a mobile robot and thus are likely expect the robot to behave within human social norms.

\section{Tour Scenario}
\label{sec:tour_scenario}

A mobile robot must first acquire the model of its environment before being tasked with navigation. In human environments, one method to familiarize the robot to a new environment is via \textit{Tour Scenario} \cite{topp2008human}. With the Tour Scenario, the robot interactively collects semantic information in an environment. Here is how the Tour Scenario is defined: After the robot product is shipped and unpacked to a home or similar environment, a human user takes the robot on a tour so the robot can create a representation of the environment. In this interactive scenario, the robot learns basic information about the surroundings, while following the user. During the tour, the user can point out and identify relevant places and objects which the robot can utilize for future tasks. It is likely that the a commercial robot product that has the capability of the familiarizing task will include both human-interactive and automatic components (i.e. automatic object recognition). In this thesis we focus on the interactive components.

\section{Semantic Maps}
\label{sec:semantic_maps}

Robots keep a representation of its environment to enable navigation behaviors, and it is usually in the form of a discrete metric \textit{map}, where each grid cells represent whether that position is occupied with an obstacle or not. Mobile robots that use such a map can accept navigation goals in metric coordinates (i.e. go to coordinate (5.2, -1.3)), however it may not be the most intuitive way to communicate goals for human users. Robots that navigate in human environments will need to accept human-friendly navigation goals. A map for such robots should act as a \textit{common ground} between the robot and its user, by enabling referencing to the same spatial elements. For example, the robot should be able to understand commands such as ``go to the kitchen table" or ``wait outside Joe's room". This is only possible with a richer map representation that includes \textit{semantic} information. In particular, if the humans and robots need to communicate over navigation goals that involve objects, landmarks or rooms, a map should include information about these.

\section{Challenges}

There are four primary challenges in developing practical systems that allow a real robotic system to navigate using semantic information. From the robot's perspective, the interesting research questions are:

\begin{itemize}
\item Where are the people?
\item How should I move around humans?
\item How can I acquire semantic information about this environment?
\item How can I utilize this semantic information to improve navigation?
\end{itemize}

The challenges laid by these questions will be addressed for the remainder of this section. The answers to the research questions are discussed throughout this thesis.

\subsection{Robust People Tracking}

In order to be engaged in any kind of meaningful spatial interaction with people, the robot has to be able to track them. Robust tracking of people is necessary to ensure high task rates.

We use a person tracking system that fuses detections from three sources. These three detectors are: leg detector, torso detector and upper body detector. Use of multiple modalities is leveraged to increase the robustness of tracking and provide more coverage around the robot. The detections are consolidated into position estimations using a Kalman Filter.

\subsection{Social Navigation}

Robot path planing for navigation has been traditionally considered as a shortest-path problem. While this leads to correct and collision-free paths, the behavior of the robot may be perceived as unsafe or unnatural by human observers. When a robot is navigating in an human environment, the most important consideration is the safety of people. Moreover, even if a robot's motions are safe, they can still cause discomfort. For example, sudden appearance of a robot can cause fear in humans or cutting in between two people while they are in a conversation may be considered a rude behavior. Therefore, the robot motions should not only be effective but also be socially acceptable.
%\label{sec:home_tour_scenario}
Borrowing the idea from Sisbot \cite{sisbot2007human}, we embed personal spaces and social constraints as costs while planning a path. We further develop the planner by considering simulating people's reaction to robot's motion during planning. Humans routinely use anticipation during navigation. For example, in a hallway encounter, one may lean towards the left of the corridor, expecting the other person to take right. We aim to give this anticipation ability to robots. Furthermore, we develop applications where the goal of the robot is to follow a person or a guide a person.

\subsection{Interactive Map Annotation}

The question of how semantic maps could be useful for various tasks is addressed in Section \ref{sec:tour_scenario}. Users may want to provide custom labels to landmarks, objects and waypoints instead of generic ones. By using unique labels, ambiguities can be resolved. For example, if an automatic object recognizer is utilized, the robot may be confused if it is given the command ``bring my laptop" and there are multiple laptops in the environment.

Moreover, the ability to label objects and then refer to them relieves the robot the burden to have a strong recognizer. Without any object recognition capability, a user can point at and label an object and later refer to it while tasking. For example, an object that was previously labeled as ``Joe's mug" can be retrieved with a command such as ``Fetch Joe's mug from the kitchen table".

A solution to enable a common ground between the user and the robot is via the Tour Scenario, as described in Section \ref{sec:tour_scenario}. For this scenario, the robot follows the user throughout the tour.

\subsection{Situational Awareness for Navigation}

Metric maps provide a single type of information: whether a grid cell is occupied with an obstacle or not. While this kind of a simplistic map representation may be sufficient for some tasks, if the robot needs to communicate with human users about its goals, it should have a richer map representation of the environment. The robot can leverage the semantic information embedded to the map and increase its usefulness for navigation tasks. %There are not many works in the field that tapped into this opportunity.

%None of the works in this field leverage the semantic maps to increase situational awareness.

Situational awareness of the robot can be achieved in different ways. Person following is a widely studied behavior, however in the literature it is not usually considered \textit{why} the robot is following the person and how the task context can be used to increase effectiveness. For example, during the Tour Scenario, the robot may employ a better following strategy if it knows the user will be pointing at landmarks and places. Furthermore, consider handling of the doors while following. If the robot does not know that it is standing on a doorway, it can block the path for others. If the door is added to the semantic map, the robot can have the opportunity to handle the door passing graciously. Moreover, the robot can utilize the map to adjust its speed. For example, if the robot is in a region where the chance to encounter a person is high, slowing down may be a sensible action to take.


%For example, when the robot is navigating, if the robot knows that it is in a region of high human density, slowing down may be a sensible action to take.



%Goal Points
%Door, landmarks, objects as special cases.
%Safety: Use map layout for speed adjustments, speed up

\section{Scope and Context}

In this section, we provide an overview of the assumptions of our research.  At a high level, autonomous robots perceive its environment, reason the situation, act in the world. In the HRI context, the research in perception is focused on improving the robot's ability to ``see" its environment, identify objects and humans. The reasoning part is focused on deciding on ``what to do" and ``how to do" in a given situation. Acting part is focused on the execution and control of motions from the reasoning part as well as the design of mechanisms. This thesis focuses on the reasoning part while also contributing to the perception part.

We assume that we have wheeled mobile robot platform. Even though some of our algorithms are derived for non-holonomic robots (differential drive), it is a technical limitation than a conceptual one, as they can be applied to holonomic robots with minor changes. We further assume that the robot is self-contained: equipped with on-board sensors, is capable of creating a map of the environment with an existing Simultaneous Localization and Mapping (SLAM) package, and then able to localize itself in this created map as it moves in the environment.

Furthermore, we focus on scenarios where people in the environment are standing or walking, because that's the most common body pose to encounter when the mobile robot is in motion.

The interaction between the robot and user is achieved via tablet or smartphone apps. Similar functionality could be achieved via a speech dialog system, however we chose touch-based interfaces due to their ubiquity and high reliability.

\section{Thesis Overview}
\subsection{Thesis Statement}

%Non-expert users can label objects, landmarks and locations while giving a tour to a mobile robot, where the labeled entities enable situational awareness and serve as end goals, which robots can navigate to, by taking into account the personal spaces of nearby humans and social factors.

Non-expert users can effortlessly interact with and control a mobile robot through the use of semantic maps and spatial rules of engagement.

\subsection{Contributions}

\begin{itemize}
%\item Development of a multimodal person detection and tracking system using on-board sensing, with a particular focus on 360$^{\circ}$ coverage
\item A method for a human to take a mobile robot on a tour and interactively add labeled objects, landmarks and locations to the robot's map using natural deictic gestures and later use these labeled entities as end goals for navigation
\item Development of a navigation planner that takes into account social factors, and reactions of people to robot's motions
\item Development of a person guidance behavior and its application to indoor wayfinding for blind users
\item Development of a person following behavior and its usage for telepresence robots
\item Demonstration of situational awareness for person following behavior, targeted at the Tour scenario
\end{itemize}


\subsection{Document Outline}

The rest of this document is organized as follows.

\textbf{Chapter \ref{chapter:map_annotation} - Map Annotation} describes our semantic map representation, the process of interactively labeling landmarks, the user interface and how we detect pointing gesture targets.

\textbf{Chapter \ref{chapter:multimodal_person_detection_and_tracking}- Multimodal Person Detection and Tracking} presents our method of leg detection, torso detection and face recognition and how the robot estimates the positions of nearby people using these detections.

\textbf{Chapter \ref{chapter:navigation_among_people} - Navigation Among People} discusses how we extract goal points from semantic maps, and a people-aware planner for navigating from a position to another.

\textbf{Chapter \ref{chapter:person_following} - Person Following} describes how the robot follows a person, uses semantic landmarks to handle special cases during following and discusses application to telepresence robots.

\textbf{Chapter \ref{chapter:person_guidance} - Person Guidance} presents a tour-guide robot and its application to particular use for blind users.

\textbf{Chapter \ref{chapter:conclusion} - Conclusion and Discussion} re-iterates contributions of this thesis and discusses open questions in this area of research.



%According to Lam~\cite{lam2011human}, mobile robots should obey certain rules while navigating in human environments. These rules include: not colliding anybody, not entering the personal space of a human unless the task is to approach the human and waiting if robot unwillingly enters the personal space of a human. Humans are already good at obeying such social conventions. Therefore most works on robot navigation in human environments is linked to human-human spatial interactions. One of the first studies in such interactions is conducted by Hall~\cite{hall1969hidden}. This study presents the proxemics theory, which categorizes the distance between people in four classes. These distances, named intimate, personal, social and public, provide spatial limits to different types of interactions. Kendon~\cite{kendon1990conducting}'s F-formation is based upon observations that people often group themselves in a spatial formation, e.g. in clusters, lines and circles. Some works adopted Hall distances and Kendon's formations for human-robot interaction. Huttenrauch~\cite{huttenrauch2006investigating} found that personal distance between a robot and a person varied in the range of 0.45 to 1.2 meters and but claimed that works of Hall and Kendon should be adapted to suit the dynamics of HRI. Avrunin~\cite{avrunin2013using} aims to learn acceptable distances from human-human experiments in an approaching scenario. 
%
%
%There are several areas of related research. The most
%closely related approach to our own is that of Human
%Augmented Mapping (HAM), introduced by Topp and Christensen
%in [23] and [22]. The Human Augmented Mapping
%approach is to have a human assist the robot in the mapping
%process, and add semantic information to the map. The
%proposed scenario is to have a human guide a robot on a tour
%of an indoor environment, adding relevant labels to the map
%throughout the tour. The HAM approach involves labeling
%two types of entities: regions, and locations. Regions are
%meant to represent areas such as rooms or hallways, and
%serve as containers for locations. Locations are meant to
%represent specific important places in the environment, such
%as a position at which a robot should perform a task. This
%approach was applied to the Cosy Explorer system, described
%in [27], which includes a semantic mapping system that
%multi-layered maps, including a metric feature based map, a
%topological map, as well as detected objects. While the goal
%of our approach is similar, we use a significantly different
%map representation, method of labeling, and interaction.




\include{chapters/map_annotation}
\include{chapters/multimodal_person_detection_and_tracking}
\include{chapters/navigation_among_people}
\include{chapters/person_following}









%%%%%%%%%%%%%%%

\section{Situation Awareness For Person Following}
\label{sec:following_situation_aware}

Most simply put, \textit{Situation Awareness (SA)} is knowing what is going on around you. Endsley \cite{endsley2000situation} defines three steps for SA: Perception is detecting the situation by perceiving cues, comprehension is combining and interpreting information and projection is forecasting future events.

In this section, we discuss SA for person following behavior for mobile robots. In the previous section, we presented the basic following behavior, where the robot follows the person from behind, while maintaining a fixed distance. The related works on person following discussed in Section \ref{sec:following_related_work} uses the same principle: the robot uses the person to calculate the target position and blindly follows the human irrespective of the situation. Although this method is sufficient for some scenarios, it can easily lead to socially awkward situations. For example, consider for a person following robot that its users stops just outside a door. In this case, the robot would occupy the doorway, blocking other people's passage, however thinks it is doing its task well because it maintains its distance to the user. If the robot knows what the person intends to do, it can anticipate those actions and suitably adjust its behavior.

Person following can be used in different contexts, such as for carrying luggage in airports or groceries in a supermarket. We showed in Chapter \ref{chapter:map_annotation} that semantic maps that include landmarks and waypoints could be to communicate goals between the robot and the user.  The stored semantic information can also be used to facilitate robot navigation. We focus on demonstration of SA for the Tour Scenario and specifically for the person following behavior.

Our method for utilizing SA for person following is via triggered events. Handling of an event during following is implemented as a sequence of four phases:

\begin{enumerate}
\item Signal: Robot detects an event using perceptual cues
\item Approach: Robot moves to a position better suited to the task
\item Task Execution: Robot and Human interacts for the task
\item Release: Robot detects the end of event
\end{enumerate}

When the event ends, robot continues with Basic Following behavior described in Section \ref{sec:following_basic_person_following}. With this methodology, robot uses the three steps of SA: Perception for detecting the start and ending of an event, Comprehension for interpreting where it should move to and what the task is, and Projection to estimate the future goals of the person. We will study the following events: how should the robot move when the user is showing landmarks in Section \ref{sec:following_landmark_labeling}, starts a conversation with another person in Section \ref{sec:following_joining_group}, and how it should handle passage of doors in Section \ref{sec:following_door_passing}.



%As another example, if the guiding person stops and chats with another person, it may be an awkward situation for the robot to wait while facing the back of its user. It may be socially more appropriate for the robot to join the group.


\subsection{Landmark Labeling}
\label{sec:following_landmark_labeling}


One of the problems during the Tour Scenario is that as the robot is following the user, it does not have any information about the task. This leads to awkward situations when the user wants to label a landmark or object, because the robot can not perceive the pointing gesture or the object/landmark of interest when it is following from behind all the time. The robot behavior can be more intelligent in those cases if the robot can predict ahead of time when the user is going to annotate map features. That way, the robot can demonstrate SA.

\begin{table}[ht!]
	\centering
  \begin{tabular}{l |  m{10cm}}    
    \toprule    
    Signal & {$dist(user, convex hull(landmark))<threshold$}\\       
	                           & {$velocity(user)\sim 0$} \\
	                           & {person roughly facing landmark}\\ \midrule		                           		                                
    Approach & {Optimal Goal: Close to both the landmark and person, facing in between}\\       \midrule
    Task Execution & {User points and labels landmark}\\  \midrule
    Release & {$dist(user, convex hull(landmark))>threshold$}\\ 
    \bottomrule
  \end{tabular}
      \caption{Conditions to trigger phases when the user is involved with the Landmark Labeling Event during following.}
    \label{table:situation_aware_list_landmark}
\end{table}


\begin{figure}[ht!]
\centering
%
        \subfigure[]{%           
           \label{fig:situtation_aware_landmark_labeling0}
           \includegraphics[width=0.475\textwidth]{pics/sit_table_00}
        } 
         \subfigure[]{%           
           \label{fig:situtation_aware_landmark_labeling2}
           \includegraphics[width=0.48\textwidth]{pics/sit_table_02}
        } \\
        \subfigure[]{%
        	\label{fig:situtation_aware_landmark_labeling3}
            \includegraphics[width=0.48\textwidth]{pics/sit_table_03}
        }%\\        
        \subfigure[]{%           
           \label{fig:situtation_aware_landmark_labeling4}
           \includegraphics[width=0.49\textwidth]{pics/sit_table_04}
        }
    \caption{Caption}
   \label{fig:situtation_aware_landmark_labeling}
\end{figure}

Our approach relies on detecting whenever labeling is going to happen, and position the robot base so it has a better chance to perceive both the pointing gesture and the object/landmark of interest. We follow the Signal/Approach/Task/Release procedure for the design of this behavior. The Signaling phase is triggered whenever the user is nearby a detected landmark or object and facing it. The user must be at full stop to enable signaling for this behavior, because the user may also walk past the landmark. After the robot detects the signal, we sample positions around the human to locate a ``suitable" goal pose. We use a utility function that scores candidate goal points. Intuitively, a pose that is close both to the landmark and person and could see both entities is considered a suitable goal pose. We sample points $360^{\circ}$ around the position of the user, for a fixed sampling resolution. Every sampled position $p$ has a score of:

\[
Score(p) = 1.0 - Cost_{visibility}(p) - Cost_{obstacle}(p)
\]

Where we define the costs as:

\begin{align} 
\begin{split} 
Cost_{visibility}(p)&=dist(person,landmark)/(dist(p,landmark) -dist(p,person)) \\
Cost_{obstacle}(p)&=max( local\_cost(p),global\_cost(p))  
\end{split} 
\end{align}

The local and global costs are 


After the goal pose is determined, the robot is commanded to navigate there. Then the user can execute the labeling task via pointing gestures. After the task is completed, the robot looks for the user to leave the landmark area. When the robot detects that the user intends to leave by checking the distance to the landmark, it continues the Tour scenario by continuing to follow the user. If, at any time during the phases, the person tracking fails, it informs the user so following can be restarted. The phases and conditions for are summarized in Table \ref{table:situation_aware_list_landmark}.

This behavior is implemented on the Segway robot for the Tour Scenario. The phases can be seen in Figure \ref{fig:situtation_aware_landmark_labeling}.




%\subsection{Joining the Group}
%\label{sec:following_joining_group}
%
%Join Group
%
%\begin{figure}[ht!]
%\centering
%\includegraphics[width=0.5\textwidth]{pics/f_formation}
%\caption{Kendon's F-Formation studies how people arrange themselves within a group}
%\label{fig:f_formation}
%\end{figure}
%
%Phases
%
%\begin{table}[H]
%	\centering
%  \begin{tabular}{l |  m{10cm}}    
%    \toprule    
%    Signal & {$dist(user, otherperson)<threshold$}\\       
%	                           & {$velocity(user)\sim 0$} \\
%	                           & {$velocity(otherperson)\sim 0$} \\ \midrule		                           		                                
%	    Approach & {Optimal Goal: inside $p \textendash space$ of the group, looking to center of the group}\\       \midrule
%    Task Execution & {Robot communicates with people, receives commands}\\  \midrule
%    Release & {$dist(user,otherperson)>threshold$}\\ 
%    \bottomrule
%  \end{tabular}
%      \caption{Conditions to trigger phases when the guiding user stops and interacts with another person during following.}
%    \label{table:situation_aware_list_group}
%\end{table}

\subsection{Door Passing}
\label{sec:following_door_passing}

\begin{table}[H]
	\centering
  \begin{tabular}{l |  m{10cm}}    
    \toprule    
    Signal & {$dist(user, convexhull(door))<threshold$}\\       
	                           & {User performs pointing gesture towards the passage}\\ \midrule   		                                	                           
    Approach & {Optimal Goal: A position on the other side of the door that doesn't block the doorway}\\       \midrule
    Task Execution & {User also passes the door}\\  \midrule
    Release & {$dist(user, convexhull(door))>threshold$ }\\ 
    \bottomrule
  \end{tabular}
      \caption{Conditions to trigger phases when the user is passing through a door during following.}
    \label{table:situation_aware_list_group}
\end{table}

\begin{figure}[ht!]
\centering
%
        \subfigure[]{%           
           \label{fig:situtation_aware_door_passing0}
           \includegraphics[width=0.475\textwidth]{pics/sit_door_00}
        } 
        \subfigure[]{%           
           \label{fig:situtation_aware_door_passing1}
           \includegraphics[width=0.48\textwidth]{pics/sit_door_01}
        } \\
        \subfigure[]{%
        	\label{fig:situtation_aware_door_passing2}
            \includegraphics[width=0.48\textwidth]{pics/sit_door_03}
        }%\\        
        \subfigure[]{%           
           \label{fig:situtation_aware_door_passing3}
           \includegraphics[width=0.49\textwidth]{pics/sit_door_04}
        }
    \caption{Caption}
   \label{fig:situtation_aware_door_passing}
\end{figure}


%%%%%%%%%%%%%%%









\include{chapters/person_guidance}


\chapter{Conclusion}
\label{chapter:conclusion}

Conclusion

Use of semantic info: generalization of tasks, increase versatility

%\nocite{*}
%% We need this since this file doesn't ACTUALLY \cite anything...
%%
\appendix

%\chapter{QR Code Based Location Initialization}
%\label{chapter:qr_code_based_location_initialization}
%
%QR Code Based Location Initialization
%
%\chapter{Assisted Remote Control}
%\label{chapter:assisted_remote_control}
%
%Assisted Remote Control
\chapter{Telepresence Study Survey Questions}
\label{chapter:telepresence_user_study_survey_questions}

\begin{figure}[ht!]
\includegraphics[width=1.0\textwidth]{pics/telepresence_survey_0}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=1.0\textwidth]{pics/telepresence_survey_1}
\end{figure}

\include{chapters/appendix_vibration_analysis}

 



\begin{postliminary}
\references

%\postfacesection{Index}{%
%%             ... generate an index here
%%         look into gatech-thesis-index.sty
%}

%Vita
%\begin{vita}
%Perry H. Disdainful was born in an insignificant town
%whose only claim to fame is that it produced such a fine
%specimen of a researcher.
%\end{vita}
\end{postliminary}

%\begin{abstract}
%  This is the abstract that must be turned in as hard copy to the
%  thesis office to meet the UMI requirements. It should \emph{not} be
%  included when submitting your ETD. Comment out the abstract
%  environment before submitting. It is recommended that you simply
%  copy and paste the text you put in the summary environment into this
%  environment. The title, your name, the page count, and your
%  advisor's name will all be generated automatically.
%\end{abstract}


\end{document}
