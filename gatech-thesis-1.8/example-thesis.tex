\documentclass[12pt]{gatech-thesis}
\usepackage{amsmath,amssymb,latexsym,float,epsfig,subfigure}
%\usepackage[round]{natbib}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
\title{People Aware Mobile Robot Navigation} %% If you want to specify a linebreak
                               %% in the thesis title, you MUST use
                               %% \protect\\ instead of \\, as \\ is a
                               %% fragile command that \MakeUpperCase
                               %% will break!
\author{Akansel Cosgun}
\department{College of Computing}

%% Can have up to six readers, plus principaladvisor and
%% committeechair. All have the form
%%
%%  \reader{Name}[Department][Institution]
%%
%% The second and third arguments are optional, but if you wish to
%% supply the third, you must supply the second. Department defaults
%% to the department defined above and Institution defaults to Georgia
%% Institute of Technology.

\principaladvisor{Professor Henrik Christensen}
\committeechair{Professor Ignatius Arrogant}
\firstreader{Professor General Reference}[School of Mathematics]
\secondreader{Professor Ivory Insular}[Department of Computer Science and Operations Research][North Dakota State University]
\thirdreader{Professor Earl Grey}
\fourthreader{Professor John Smith}
\fifthreader{Professor Jane Doe}[Another Department With a Long Name][Another Institution]
%\setcounter{secnumdepth}{2}
\degree{Doctor of Philosophy}

%% Set \listmajortrue below, then uncomment and set this for
%% interdisciplinary PhD programs so that the title page says
%% ``[degree] in [major]'' and puts the department at the bottom of
%% the page, rather than saying ``[degree] in the [department]''

%% \major{Algorithms, Combinatorics, and Optimization} 

\copyrightyear{2010} 
\submitdate{August 2010} % Must be the month and year of graduation,
                         % not thesis approval! As of 2010, this means
                         % this text must be May, August, or December
                         % followed by the year.

%% The date the last committee member signs the thesis form. Printed
%% on the approval page.
\approveddate{1 July 2010}


\bibfiles{example-thesis}

%% The following are the defaults
%%    \titlepagetrue
%%    \signaturepagetrue
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
%%    \bibpagetrue
%%    \thesisproposalfalse
%%    \strictmarginstrue
%%    \dissertationfalse
%%    \listmajorfalse
%%    \multivolumefalse

\begin{document}
%\bibliographystyle{plainnat}
\bibliographystyle{gatech-thesis}
%%
\begin{preliminary}
\begin{dedication}
\null\vfil
{\large
\begin{center}
To myself,\\\vspace{12pt}
Perry H. Disdainful,\\\vspace{12pt}
the only person worthy of my company.
\end{center}}
\vfil\null
\end{dedication}
\begin{preface}
Theses have elements.  Isn't that nice?
\end{preface}
\begin{acknowledgements}
I want to thank people
\end{acknowledgements}
% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.
\begin{summary}
Why should I provide a summary?  Just read the thesis.
\end{summary}
\end{preliminary}
%%
\chapter{Introduction}

Introduction

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Map Annotation}

Map Annotation
  
\section{Related Work}

Related Work

\section{Semantic Maps}

Semantic Maps

\subsection{Waypoints}

\subsection{Planar Landmarks}

\subsection{Objects}


\section{User Interface}

User Interface


\section{Pointing Gestures for Human-Robot Interaction}

Pointing Gestures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Navigation Among People}

Autonomous Robot Navigation

\section{Related Work}

Related Work

\section{State of Autonomous Robot Navigation}

State of Autonomous Robot Navigation

\section{Finding Goal Points for Navigation}

Finding Goal Points for Navigation

\section{People Aware Navigation}

People Aware Navigation

\section{Speed Maps for Safe Navigation}

Speed Maps for Safer Navigation


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Multimodal Person Tracking}

The ability to robustly track a person is an important prerequisite for human-robot interaction. To realize any task that involves humans, the challenge is the detection and tracking of humans in the vicinity of the robot considering the robot's movements, sensing capabilities and occlusions. The scope of how much information is needed from the human perception module depends on the objective of the application. First, the robot should determine if there are people nearby. If the robot senses people around, the robot should find out \emph{where} they are. Representing people as points (x,y) in maps is common practice for navigation planning. If the task requires the robot to face a person, then the orientation $\theta$ needs be detected. The robot further can determine \emph{who} the detected person is. Identification of humans is necessary for enabling non-generic service. Finally, the robot should interpret \emph{what} the person is doing by analyzing the motion features and through gesture analysis. Tracking body parts of humans over time give significant information about human activity.

We focus on tracking people who are either walking or standing, as these are the two most common human poses around a mobile robot. Many full-body or body part detectors have been developed in the literature, reviewed in Section~\ref{sec:multimodal_related_work}. Full-body detectors are not suitable for mobile robot navigation applications because of their inability of capturing the entire body with on-board sensors when people are close to the robot. We aim to robustly track a person $360^{\circ}$ around the robot. However, most sensors have a limited field of view and using only a single detector can lead to a system with a single point of failure. Therefore, we think a multimodal detection system is better suited for on-board people tracking for our use cases. We use a combination of detectors using laser scanners and RGB-D cameras for robustness and better coverage, described in Section~\ref{sec:multimodal_person_detection}. Representing people as a points in the map is sufficient for mobile robot navigation and each detector produces a point as a person hypothesis. We use a real-time probabilistic tracking framework that relies on the fusion of the multiple person detections, described in Section~\ref{sec:multimodal_person_tracking}. For certain applications, identifying specific users allows the robot to go beyond generic capabilities. We present our face recognition method in Section~\ref{sec:multimodal_face_recognition}.

\section{Related Work}
\label{sec:multimodal_related_work}

Person detection was first addressed by the computer vision community as an object detection problem. Early research on person detection using vision is surveyed by Moeslund \cite{moeslund2001}. Face detection is a common method for detecting people, with the work of Viola and Jones \cite{viola2004robust} being the most popular one. See Zhang \cite{zhang2010survey} for a survey on contemporary approaches on vision based face detection. Another popular topic has been pedestrian detection in crowded scenes Leibe \cite{leibe2005pedestrian} and Tuzel \cite{tuzel2007human}.

In 2000's, laser scanners became the de-facto sensor for localization and mapping. Laser scanners are usually placed slightly above floor for obstacle avoidance, therefore leg detection is common practice. Early works by Montemerlo \cite{montemerlo2002conditional} and Schulz \cite{schulz2001tracking} focused on tracking multiple legs using particle filters. Legs are typically distinguished in laser scans using geometric features such as arcs \cite{xavier2005fast} and boosting can be used to train a classifier on a multitude of features \cite{arras2007using}. Topp \cite{topp2005tracking} demonstrates that leg tracking in cluttered environments is prone to false positives. For more robust tracking, some efforts fused information from multiple lasers such as Carballo's work \cite{Carballo2008}, which uses a second laser scanner at torso level. Glas \cite{glas2009laser} uses a network of laser sensors at torso height in hall-type environments to track the position and body orientation of multiple people. Several works used different modalities of sensors to further improve the robustness. Kleinehagenbrock \cite{kleinehagenbrock2002person} and Bellotto \cite{bellotto2009multisensor} combine leg detection and face tracking in a multi-modal tracking framework. Other examples include combining sound localization and vision \cite{bernardin2007audio} and combining RFID tracking and vision \cite{germa2010vision}.

Laser-based person methods pertains tracking of humans in 2D, projected to floor plane. Tracking of the body parts has long been a topic of interest in vision \cite{baumberg1997learning,sidenbladh2000stochastic}. With the recent introduction of 3D sensors such as the Velodyne, Swissranger and Kinect, more robust tracking became possible. Spinello \cite{spinello2010layered} trains geometrical features at different height levels in the 3D point cloud for pedestrian detection. Ganapathi \cite{ganapathi2010real} estimates body part locations with a probabilistic model. One of the well-known skeleton tracking algorithms is the Microsoft Kinect SDK by Shotton \cite{shotton2013real}, which trains decision forests using simple depth features in a vast database. This software is not suitable to work on a mobile robot as it is designed to work on a stationary sensor. In the robotics community, there are efforts to develop skeleton trackers that work on mobile robots and in unstructured scenes \cite{buys2013adaptable}.

We use standard laser-based leg tracking, which is usually sufficient for most of our applications. Its wide field of view is also advantageous than cameras, because humans can be anywhere around the robot for interactive navigation. However, orientation of the person can not be detected reliably if the laser is at leg level because there aren't many discriminant features in resulting scans. Usually, the velocity of the leg segment, after some filtering operation, is accepted as the orientation of the person. We, therefore, use a torso-level laser scanner for tasks that required detection of orientation. We also make use of 3$^{rd}$ party skeleton tracking software to detect gestures (i.e. pointing). Because estimation of the pointing direction highly depends on the tracking error, it is valuable to make an accuracy analysis of existing skeleton trackers.

Face recognition is a widely used application as surveyed by Phillips \cite{phillips2005overview}. One of the pioneers in face recognition uses a set of patch masks for features that doesn't necessarily correspond to eyes, ears or noses \cite{turk1991face}. \cite{zhao1998discriminant} combines PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis) to improve the generalization capability when only a few samples are available.

There has been some work to identify humans using 3D data, such as the head-to-shoulder signature \cite{kirchner2012head} and body motion characteristics \cite{munsell2012person}. Biometric person identification techniques, such speaker recognition \cite{kinnunen2010overview}, 3D ear shape \cite{yan2007biometric} and multi-modal cues \cite{garcia2003biomet} have potential to be more accurate than face recognition. However, these approaches are better suited to work in controlled environments.

\section{Person Detection}
\label{sec:multimodal_person_detection}

Person Detection

\subsection{Leg Detection}

Laser scanners are the natural sensor of choice as state-of-the-art mobile robots are already equipped with an ankle-height laser scanner that is mainly used for navigation. 

\subsection{Torso Detection}

Leg Detection

\subsection{Lower Body Detection}

Lower Body Detection

\section{Person Tracking}
\label{sec:multimodal_person_tracking}

Multimodal Person Tracking

\section{Face Recognition}
\label{sec:multimodal_face_recognition}

Face Recognition

%A possible use of person recognition in interactive navigation is to drive up to a specific person. Another possible use case for our work is to re-initialize person following if the user is lost. We have used face recognition in some of our applications, however it requires the face of the person to be visible. Vision community works on datasets that are selected nicely as pictures taken by humans are likely to have more faces in it. On the other hand, most frames the robot acquires won't have any faces in it. Development of person recognition approaches that are suited for on-board sensing on mobile systems is an open research area.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Person Following}

Person Following

\section{Related Work}

Related Work

\section{Basic Person Following}

Basic Person Following

\section{Situation Aware Person Following}

Situation Aware Person Following

\subsection{Door Passing}

\subsection{User Activity Awareness}

\subsection{Corners}


\section{Application To Telepresence Robots}

Application To Telepresence Robots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Person Guidance}

Person Guidance

\section{Related Work}

Related Work

\section{Guide Robot}

Guide Robot

\section{Application To Blind Users}

Application To Blind Users

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}

Conclusion


\begin{table}
\caption{A table, centered.}
\begin{center}
\begin{tabular}{|l|r|}
  \hline 
Title & Author \\
\hline
War And Peace & Leo Tolstoy \\
The Great Gatsby & F. Scott Fitzgerald \\ \hline
\end{tabular}
\end{center}
\end{table}
%%





%\nocite{*}
%% We need this since this file doesn't ACTUALLY \cite anything...
%%
\appendix
\chapter{QR Code Based Location Initialization}

QR Code Based Location Initialization

\chapter{Assisted Remote Control}

Assisted Remote Control

\chapter{Vibration Pattern Analysis for Haptic Belts}

Vibration Pattern Analysis for Haptic Belts


\begin{postliminary}
\references
\postfacesection{Index}{%
%%             ... generate an index here
%%         look into gatech-thesis-index.sty
}
\begin{vita}
Perry H. Disdainful was born in an insignificant town
whose only claim to fame is that it produced such a fine
specimen of a researcher.
\end{vita}
\end{postliminary}

\begin{abstract}
  This is the abstract that must be turned in as hard copy to the
  thesis office to meet the UMI requirements. It should \emph{not} be
  included when submitting your ETD. Comment out the abstract
  environment before submitting. It is recommended that you simply
  copy and paste the text you put in the summary environment into this
  environment. The title, your name, the page count, and your
  advisor's name will all be generated automatically.
\end{abstract}

\end{document}
